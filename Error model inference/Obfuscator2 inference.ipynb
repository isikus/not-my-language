{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Obfuscator inference","provenance":[{"file_id":"1cSB8SNt7AO943Sg4wdzyENR-wPYdi_N5","timestamp":1586174293011}],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyP/3/0N1ylOoKx5EqC+A2+t"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"krWE6guj2QzA","colab_type":"code","outputId":"8cf07d2f-8154-423c-e91c-e1b33d48a7c3","executionInfo":{"status":"ok","timestamp":1586169530209,"user_tz":-180,"elapsed":1586,"user":{"displayName":"isaac truitt","photoUrl":"","userId":"17593568044731677741"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%tensorflow_version 1.x"],"execution_count":1,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uH2hotka4xuc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"a86e4c43-bf38-47c9-8ef7-5b8cc695b048","executionInfo":{"status":"ok","timestamp":1586169530932,"user_tz":-180,"elapsed":2259,"user":{"displayName":"isaac truitt","photoUrl":"","userId":"17593568044731677741"}}},"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\")"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ur_W654Vb_Ss","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","import os\n","from copy import deepcopy\n","from os import listdir\n","from os.path import isfile, join\n","from collections import namedtuple\n","from tensorflow.python.layers.core import Dense\n","from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n","import time\n","import re\n","from sklearn.model_selection import train_test_split"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LX1jmfc4rorg","colab_type":"code","outputId":"4613684e-e130-403c-8739-3d70aed7d993","executionInfo":{"status":"ok","timestamp":1586169534164,"user_tz":-180,"elapsed":5386,"user":{"displayName":"isaac truitt","photoUrl":"","userId":"17593568044731677741"}},"colab":{"base_uri":"https://localhost:8080/","height":336}},"source":["!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1-8EW1wlmn9n9ikbTNowqKoiVIhEJoeJP' -O spelling_errs.pickle"],"execution_count":4,"outputs":[{"output_type":"stream","text":["--2020-04-06 10:38:52--  https://docs.google.com/uc?export=download&id=1-8EW1wlmn9n9ikbTNowqKoiVIhEJoeJP\n","Resolving docs.google.com (docs.google.com)... 172.217.212.102, 172.217.212.100, 172.217.212.113, ...\n","Connecting to docs.google.com (docs.google.com)|172.217.212.102|:443... connected.\n","HTTP request sent, awaiting response... 302 Moved Temporarily\n","Location: https://doc-0g-54-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/a4jauibags48dm9g10m3tk6e9rd0bacs/1586169525000/07481003931234110333/*/1-8EW1wlmn9n9ikbTNowqKoiVIhEJoeJP?e=download [following]\n","Warning: wildcards not supported in HTTP.\n","--2020-04-06 10:38:53--  https://doc-0g-54-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/a4jauibags48dm9g10m3tk6e9rd0bacs/1586169525000/07481003931234110333/*/1-8EW1wlmn9n9ikbTNowqKoiVIhEJoeJP?e=download\n","Resolving doc-0g-54-docs.googleusercontent.com (doc-0g-54-docs.googleusercontent.com)... 209.85.146.132, 2607:f8b0:4001:c1f::84\n","Connecting to doc-0g-54-docs.googleusercontent.com (doc-0g-54-docs.googleusercontent.com)|209.85.146.132|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [application/octet-stream]\n","Saving to: ‘spelling_errs.pickle’\n","\n","\rspelling_errs.pickl     [<=>                 ]       0  --.-KB/s               \rspelling_errs.pickl     [ <=>                ]   3.86M  --.-KB/s    in 0.02s   \n","\n","2020-04-06 10:38:53 (173 MB/s) - ‘spelling_errs.pickle’ saved [4046241]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zZK9y-lIp3VK","colab":{}},"source":["spelling_errs = pd.read_pickle(\"spelling_errs.pickle\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ojgmVq8BZRhZ","colab_type":"code","colab":{}},"source":["test_frac = 0.1\n","\n","spelling_errs = spelling_errs.sample(frac=1, random_state=42).reset_index(drop=True)\n","test_df = spelling_errs[:int(len(spelling_errs) * test_frac)]\n","spelling_errs = spelling_errs[int(len(spelling_errs) * test_frac):]\n","\n","test_df.to_pickle(\"test.pickle\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"m02xOpK-15G_","colab_type":"code","colab":{}},"source":["errors = [word.lower() for word in list(spelling_errs[\"Ошибка\"])]\n","corrections = [word.lower() for word in list(spelling_errs[\"Исправление\"])]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"O9wnSpMmPrMh","colab_type":"code","outputId":"0477205d-5781-42bc-82fc-09d1cb254644","executionInfo":{"status":"ok","timestamp":1586169534494,"user_tz":-180,"elapsed":5598,"user":{"displayName":"isaac truitt","photoUrl":"","userId":"17593568044731677741"}},"colab":{"base_uri":"https://localhost:8080/","height":193}},"source":["for p in zip(errors[:10], corrections[:10]):\n","  print(p[0], p[1], sep=\"\\t\")"],"execution_count":8,"outputs":[{"output_type":"stream","text":["штейн\tстайн\n","не   многие\tнемногие\n","енергии\tэнергии\n","беспокоит\tбеспокоить\n","развывающиеся\tразвивающиеся\n","не однородный\tнеоднородный\n","левой\tлевые\n","епигона\tэпигона\n","элизавет\tэлизабет\n","науке против старения\tнауке против старения.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yQQl9aEgb_Te","colab_type":"text"},"source":["## Preparing the Data"]},{"cell_type":"code","metadata":{"id":"7uEnZw0jb_Tg","colab_type":"code","colab":{}},"source":["def clean_text(text):\n","    '''Remove unwanted characters and extra spaces from the text'''\n","    return text.lower()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JLNAx0fTb_Tt","colab_type":"code","colab":{}},"source":["# Clean the text of the books\n","clean_books = []\n","for p in zip(errors, corrections):\n","  clean_books.append([clean_text(p[1]), clean_text(p[0])])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tlgQSfAcb_US","colab_type":"code","colab":{}},"source":["# Create a dictionary to convert the vocabulary (characters) to integers\n","vocab_to_int = {}\n","count = 0\n","for book in clean_books:\n","    for character in book[0]+book[1]:\n","        if character not in vocab_to_int:\n","            vocab_to_int[character] = count\n","            count += 1\n","\n","# Add special tokens to vocab_to_int\n","codes = ['<PAD>','<EOS>','<GO>']\n","for code in codes:\n","    vocab_to_int[code] = count\n","    count += 1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5kIw8DXdb_Uh","colab_type":"code","outputId":"df4b9f80-b502-4e68-822d-268e2c1dab03","executionInfo":{"status":"ok","timestamp":1586169534497,"user_tz":-180,"elapsed":5501,"user":{"displayName":"isaac truitt","photoUrl":"","userId":"17593568044731677741"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["# Check the size of vocabulary and all of the values\n","vocab_size = len(vocab_to_int)\n","print(\"The vocabulary contains {} characters.\".format(vocab_size))\n","print(sorted(vocab_to_int))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["The vocabulary contains 98 characters.\n","[' ', '!', '\"', '#', '%', '&', \"'\", '(', ')', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '<EOS>', '<GO>', '<PAD>', '>', '?', 'a', 'b', 'c', 'd', 'e', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\xa0', '«', '»', 'è', 'ë', '́', 'а', 'б', 'в', 'г', 'д', 'е', 'ж', 'з', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', 'я', 'ё', 'қ', '–', '—', '’', '№']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"30GvI1Elb_Uq","colab_type":"text"},"source":["*Note: We could have made this project a little easier by using only lower case words and fewer special characters ($,&,-...), but I want to make this spell checker as useful as possible.*"]},{"cell_type":"code","metadata":{"id":"PzBpQntTb_Uu","colab_type":"code","colab":{}},"source":["# Create another dictionary to convert integers to their respective characters\n","int_to_vocab = {}\n","for character, value in vocab_to_int.items():\n","    int_to_vocab[value] = character"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"g8l2TDjJb_U5","colab_type":"code","outputId":"abc7de92-9ae5-45a8-aff7-1d174d63b65a","executionInfo":{"status":"ok","timestamp":1586169534498,"user_tz":-180,"elapsed":5438,"user":{"displayName":"isaac truitt","photoUrl":"","userId":"17593568044731677741"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Split the text from the books into sentences.\n","sentences = deepcopy(clean_books)\n","print(\"There are {} sentences.\".format(len(sentences)))"],"execution_count":14,"outputs":[{"output_type":"stream","text":["There are 8470 sentences.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5j2F2ysNb_VE","colab_type":"code","colab":{}},"source":["# Convert sentences to integers\n","int_sentences = []\n","\n","for sentence in sentences:\n","    int_sentence = []\n","    for character in sentence[0]:\n","        int_sentence.append(vocab_to_int[character])\n","    int_sentences.append(int_sentence)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bpr01oT7b_VZ","colab_type":"code","outputId":"41c885a5-d9d7-4fd4-a49e-a8fc194d55b4","executionInfo":{"status":"ok","timestamp":1586169534499,"user_tz":-180,"elapsed":5386,"user":{"displayName":"isaac truitt","photoUrl":"","userId":"17593568044731677741"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Find the length of each sentence\n","lengths = []\n","for sentence in int_sentences:\n","    lengths.append(len(sentence))\n","lengths = pd.DataFrame(lengths, columns=[\"counts\"])\n","\n","# Limit the data we will use to train our model\n","max_length = int(lengths.describe().loc[\"75%\"])+1\n","min_length = 2\n","\n","good_sentences = []\n","\n","for sentence in int_sentences:\n","    if len(sentence) <= max_length and len(sentence) >= min_length:\n","        good_sentences.append(sentence)\n","\n","print(\"We will use {} to train and test our model.\".format(len(good_sentences)))"],"execution_count":16,"outputs":[{"output_type":"stream","text":["We will use 7383 to train and test our model.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Y4Y8Hx8eb_Vf","colab_type":"text"},"source":["*Note: I decided to not use very long or short sentences because they are not as useful for training our model. Shorter sentences are less likely to include an error and the text is more likely to be repetitive. Longer sentences are more difficult to learn due to their length and increase the training time quite a bit. If you are interested in using this model for more than just a personal project, it would be worth using these longer sentence, and much more training data to create a more accurate model.*"]},{"cell_type":"code","metadata":{"id":"t2XdN-1cb_Vh","colab_type":"code","outputId":"f471ade1-6cb1-4b23-cf2e-2f6bf0bf64e6","executionInfo":{"status":"ok","timestamp":1586169534499,"user_tz":-180,"elapsed":5345,"user":{"displayName":"isaac truitt","photoUrl":"","userId":"17593568044731677741"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["# Split the data into training and testing sentences\n","training, testing = train_test_split(good_sentences, test_size = 0.15, random_state = 42)\n","\n","print(\"Number of training sentences:\", len(training))\n","print(\"Number of testing sentences:\", len(testing))"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Number of training sentences: 6275\n","Number of testing sentences: 1108\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wJdcHaw5b_Vo","colab_type":"code","colab":{}},"source":["# Sort the sentences by length to reduce padding, which will allow the model to train faster\n","training_sorted = []\n","testing_sorted = []\n","\n","for i in range(min_length, max_length+1):\n","    for sentence in training:\n","        if len(sentence) == i:\n","            training_sorted.append(sentence)\n","    for sentence in testing:\n","        if len(sentence) == i:\n","            testing_sorted.append(sentence)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PAPRoXsyb_Vw","colab_type":"code","outputId":"b236168f-5dce-4c7c-eb3f-23afd2fc8ab8","executionInfo":{"status":"ok","timestamp":1586169534500,"user_tz":-180,"elapsed":5317,"user":{"displayName":"isaac truitt","photoUrl":"","userId":"17593568044731677741"}},"colab":{"base_uri":"https://localhost:8080/","height":105}},"source":["# Check to ensure the sentences have been selected and sorted correctly\n","for i in range(5):\n","    print(training_sorted[i], len(training_sorted[i]))"],"execution_count":19,"outputs":[{"output_type":"stream","text":["[4, 8] 2\n","[10, 18] 2\n","[6, 41] 2\n","[4, 2] 2\n","[18, 2] 2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"y5XaefnyEQEf","colab_type":"code","colab":{}},"source":["def decoding_seq(int_sequence):\n","  return \"\".join([int_to_vocab[i] for i in int_sequence])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0oavT5LIb_V9","colab_type":"code","colab":{}},"source":["inps = [p[0] for p in clean_books]\n","noise = [p[1] for p in clean_books]\n","\n","training_noisy = []\n","for seq in training_sorted:\n","  training_noisy.append([vocab_to_int[c] for c in noise[inps.index(decoding_seq(seq))]])\n","\n","testing_noisy = []\n","for seq in testing_sorted:\n","  testing_noisy.append([vocab_to_int[c] for c in noise[inps.index(decoding_seq(seq))]])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"5OhavmMcb_WO","colab_type":"text"},"source":["# Building the Model"]},{"cell_type":"code","metadata":{"id":"0yc7VJC0b_WQ","colab_type":"code","colab":{}},"source":["def model_inputs():\n","    '''Create palceholders for inputs to the model'''\n","    \n","    with tf.name_scope('inputs'):\n","        inputs = tf.placeholder(tf.int32, [None, None], name='inputs')\n","    with tf.name_scope('targets'):\n","        targets = tf.placeholder(tf.int32, [None, None], name='targets')\n","    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n","    inputs_length = tf.placeholder(tf.int32, (None,), name='inputs_length')\n","    targets_length = tf.placeholder(tf.int32, (None,), name='targets_length')\n","    max_target_length = tf.reduce_max(targets_length, name='max_target_len')\n","\n","    return inputs, targets, keep_prob, inputs_length, targets_length, max_target_length"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"g5W49COfb_Wd","colab_type":"code","colab":{}},"source":["def process_encoding_input(targets, vocab_to_int, batch_size):\n","    '''Remove the last word id from each batch and concat the <GO> to the begining of each batch'''\n","    \n","    with tf.name_scope(\"process_encoding\"):\n","        ending = tf.strided_slice(targets, [0, 0], [batch_size, -1], [1, 1])\n","        dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n","\n","    return dec_input"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EYTg52tFb_Wk","colab_type":"code","colab":{}},"source":["def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob, direction):\n","    '''Create the encoding layer'''\n","    \n","    if direction == 1:\n","        with tf.name_scope(\"RNN_Encoder_Cell_1D\"):\n","            for layer in range(num_layers):\n","                with tf.variable_scope('encoder_{}'.format(layer)):\n","                    lstm = tf.contrib.rnn.LSTMCell(rnn_size)\n","\n","                    drop = tf.contrib.rnn.DropoutWrapper(lstm, \n","                                                         input_keep_prob = keep_prob)\n","\n","                    enc_output, enc_state = tf.nn.dynamic_rnn(drop, \n","                                                              rnn_inputs,\n","                                                              sequence_length,\n","                                                              dtype=tf.float32)\n","\n","            return enc_output, enc_state\n","        \n","        \n","    if direction == 2:\n","        with tf.name_scope(\"RNN_Encoder_Cell_2D\"):\n","            for layer in range(num_layers):\n","                with tf.variable_scope('encoder_{}'.format(layer)):\n","                    cell_fw = tf.contrib.rnn.LSTMCell(rnn_size)\n","                    cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \n","                                                            input_keep_prob = keep_prob)\n","\n","                    cell_bw = tf.contrib.rnn.LSTMCell(rnn_size)\n","                    cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \n","                                                            input_keep_prob = keep_prob)\n","\n","                    enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n","                                                                            cell_bw, \n","                                                                            rnn_inputs,\n","                                                                            sequence_length,\n","                                                                            dtype=tf.float32)\n","            # Join outputs since we are using a bidirectional RNN\n","            enc_output = tf.concat(enc_output,2)\n","            # Use only the forward state because the model can't use both states at once\n","            return enc_output, enc_state[0]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OOPkZ568b_Wr","colab_type":"code","colab":{}},"source":["def training_decoding_layer(dec_embed_input, targets_length, dec_cell, initial_state, output_layer, \n","                            vocab_size, max_target_length):\n","    '''Create the training logits'''\n","    \n","    with tf.name_scope(\"Training_Decoder\"):\n","        training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n","                                                            sequence_length=targets_length,\n","                                                            time_major=False)\n","\n","        training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n","                                                           training_helper,\n","                                                           initial_state,\n","                                                           output_layer) \n","\n","        training_logits, _ ,_ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n","                                                                    output_time_major=False,\n","                                                                    impute_finished=True,\n","                                                                    maximum_iterations=max_target_length)\n","        return training_logits"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ifuDRcyrb_Wv","colab_type":"code","colab":{}},"source":["def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, initial_state, output_layer,\n","                             max_target_length, batch_size):\n","    '''Create the inference logits'''\n","    \n","    with tf.name_scope(\"Inference_Decoder\"):\n","        start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n","\n","        inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n","                                                                    start_tokens,\n","                                                                    end_token)\n","\n","        inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n","                                                            inference_helper,\n","                                                            initial_state,\n","                                                            output_layer)\n","\n","        inference_logits, _ ,_ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n","                                                                    output_time_major=False,\n","                                                                    impute_finished=True,\n","                                                                    maximum_iterations=max_target_length)\n","\n","        return inference_logits"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jAADlTiUb_W2","colab_type":"code","colab":{}},"source":["def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, inputs_length, targets_length, \n","                   max_target_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers, direction):\n","    '''Create the decoding cell and attention for the training and inference decoding layers'''\n","    \n","    with tf.name_scope(\"RNN_Decoder_Cell\"):\n","        for layer in range(num_layers):\n","            with tf.variable_scope('decoder_{}'.format(layer)):\n","                lstm = tf.contrib.rnn.LSTMCell(rnn_size)\n","                dec_cell = tf.contrib.rnn.DropoutWrapper(lstm, \n","                                                         input_keep_prob = keep_prob)\n","    \n","    output_layer = Dense(vocab_size,\n","                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n","    \n","    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n","                                                  enc_output,\n","                                                  inputs_length,\n","                                                  normalize=False,\n","                                                  name='BahdanauAttention')\n","    \n","    with tf.name_scope(\"Attention_Wrapper\"):\n","        dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell,\n","                                                      attn_mech,\n","                                                      rnn_size)\n","    \n","    initial_state = dec_cell.zero_state(batch_size=batch_size,dtype=tf.float32).clone(cell_state=enc_state)\n","\n","    with tf.variable_scope(\"decode\"):\n","        training_logits = training_decoding_layer(dec_embed_input, \n","                                                  targets_length, \n","                                                  dec_cell, \n","                                                  initial_state,\n","                                                  output_layer,\n","                                                  vocab_size, \n","                                                  max_target_length)\n","    with tf.variable_scope(\"decode\", reuse=True):\n","        inference_logits = inference_decoding_layer(embeddings,  \n","                                                    vocab_to_int['<GO>'], \n","                                                    vocab_to_int['<EOS>'],\n","                                                    dec_cell, \n","                                                    initial_state, \n","                                                    output_layer,\n","                                                    max_target_length,\n","                                                    batch_size)\n","\n","    return training_logits, inference_logits"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GJX_hK8ob_W6","colab_type":"code","colab":{}},"source":["def seq2seq_model(inputs, targets, keep_prob, inputs_length, targets_length, max_target_length, \n","                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size, embedding_size, direction):\n","    '''Use the previous functions to create the training and inference logits'''\n","    \n","    enc_embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1, 1))\n","    enc_embed_input = tf.nn.embedding_lookup(enc_embeddings, inputs)\n","    enc_output, enc_state = encoding_layer(rnn_size, inputs_length, num_layers, \n","                                           enc_embed_input, keep_prob, direction)\n","    \n","    dec_embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1, 1))\n","    dec_input = process_encoding_input(targets, vocab_to_int, batch_size)\n","    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n","    \n","    training_logits, inference_logits  = decoding_layer(dec_embed_input, \n","                                                        dec_embeddings,\n","                                                        enc_output,\n","                                                        enc_state, \n","                                                        vocab_size, \n","                                                        inputs_length, \n","                                                        targets_length, \n","                                                        max_target_length,\n","                                                        rnn_size, \n","                                                        vocab_to_int, \n","                                                        keep_prob, \n","                                                        batch_size,\n","                                                        num_layers,\n","                                                        direction)\n","    \n","    return training_logits, inference_logits"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"P9k9z9ztb_W-","colab_type":"code","colab":{}},"source":["def pad_sentence_batch(sentence_batch):\n","    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n","    max_sentence = max([len(sentence) for sentence in sentence_batch])\n","    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xGxUa8_eb_XK","colab_type":"code","colab":{}},"source":["# The default parameters\n","epochs = 100\n","batch_size = 128\n","num_layers = 2\n","rnn_size = 512\n","embedding_size = 128\n","learning_rate = 0.0005\n","direction = 2\n","threshold = 0.95\n","keep_probability = 0.75"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"w2sLe5tCb_XS","colab_type":"code","colab":{}},"source":["def build_graph(keep_prob, rnn_size, num_layers, batch_size, learning_rate, embedding_size, direction):\n","\n","    tf.reset_default_graph()\n","    \n","    # Load the model inputs    \n","    inputs, targets, keep_prob, inputs_length, targets_length, max_target_length = model_inputs()\n","\n","    # Create the training and inference logits\n","    training_logits, inference_logits = seq2seq_model(tf.reverse(inputs, [-1]),\n","                                                      targets, \n","                                                      keep_prob,   \n","                                                      inputs_length,\n","                                                      targets_length,\n","                                                      max_target_length,\n","                                                      len(vocab_to_int)+1,\n","                                                      rnn_size, \n","                                                      num_layers, \n","                                                      vocab_to_int,\n","                                                      batch_size,\n","                                                      embedding_size,\n","                                                      direction)\n","\n","    # Create tensors for the training logits and inference logits\n","    training_logits = tf.identity(training_logits.rnn_output, 'logits')\n","\n","    with tf.name_scope('predictions'):\n","        predictions = tf.identity(inference_logits.sample_id, name='predictions')\n","        tf.summary.histogram('predictions', predictions)\n","\n","    # Create the weights for sequence_loss\n","    masks = tf.sequence_mask(targets_length, max_target_length, dtype=tf.float32, name='masks')\n","    \n","    with tf.name_scope(\"cost\"):\n","        # Loss function\n","        cost = tf.contrib.seq2seq.sequence_loss(training_logits, \n","                                                targets, \n","                                                masks)\n","        tf.summary.scalar('cost', cost)\n","\n","    with tf.name_scope(\"optimze\"):\n","        optimizer = tf.train.AdamOptimizer(learning_rate)\n","\n","        # Gradient Clipping\n","        gradients = optimizer.compute_gradients(cost)\n","        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n","        train_op = optimizer.apply_gradients(capped_gradients)\n","\n","    # Merge all of the summaries\n","    merged = tf.summary.merge_all()    \n","\n","    # Export the nodes \n","    export_nodes = ['inputs', 'targets', 'keep_prob', 'cost', 'inputs_length', 'targets_length',\n","                    'predictions', 'merged', 'train_op','optimizer']\n","    Graph = namedtuple('Graph', export_nodes)\n","    local_dict = locals()\n","    graph = Graph(*[local_dict[each] for each in export_nodes])\n","\n","    return graph"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"YzwSIeNGb_Xi","colab_type":"text"},"source":["## Fixing Custom Sentences"]},{"cell_type":"code","metadata":{"id":"3nwMImkNb_Xj","colab_type":"code","colab":{}},"source":["def text_to_ints(text, stoi):\n","    '''Prepare the text for the model'''\n","    \n","    text = text.lower()\n","    ret_ints = []\n","    for char in text:\n","      try:\n","        ret_ints.append(stoi[char])\n","      except KeyError:\n","        ret_ints.append(stoi['<PAD>'])\n","    return ret_ints"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pNPGcybRvXNt","colab_type":"code","colab":{}},"source":["def get_batches(sentences, batch_size):\n","    \"\"\"Batch sentences, noisy sentences, and the lengths of their sentences together.\n","       With each epoch, sentences will receive new mistakes\"\"\"\n","    \n","    for batch_i in range(0, len(sentences)//batch_size):\n","        start_i = batch_i * batch_size\n","        sentences_batch = sentences[start_i:start_i + batch_size]\n","            \n","        sentences_batch_eos = []\n","        for sentence in sentences_batch:\n","            sentence.append(vocab_to_int['<EOS>'])\n","            sentences_batch_eos.append(sentence)\n","            \n","        pad_sentences_batch = np.array(pad_sentence_batch(sentences_batch_eos))\n","        \n","        # Need the lengths for the _lengths parameters\n","        pad_sentences_lengths = []\n","        for sentence in pad_sentences_batch:\n","            pad_sentences_lengths.append(len(sentence))\n","        \n","        yield pad_sentences_batch, pad_sentences_lengths"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZXkgxcfajgpN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"7b93eeaf-6c3f-4877-e5ee-869ab125f00a","executionInfo":{"status":"ok","timestamp":1586169556498,"user_tz":-180,"elapsed":27004,"user":{"displayName":"isaac truitt","photoUrl":"","userId":"17593568044731677741"}}},"source":["%%time\n","\n","leipzig_df = pd.read_pickle(\"/content/gdrive/My Drive/RLC2/leipzig_df.pickle\")\n","selected_words = list(leipzig_df.loc[leipzig_df[\"error\"]][\"word\"])\n","del leipzig_df"],"execution_count":34,"outputs":[{"output_type":"stream","text":["CPU times: user 16.6 s, sys: 4.37 s, total: 21 s\n","Wall time: 21.2 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LoMrIFUEj5Gl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"48a22310-1445-4a93-f63f-aff33c46b5c6","executionInfo":{"status":"ok","timestamp":1586169556498,"user_tz":-180,"elapsed":26993,"user":{"displayName":"isaac truitt","photoUrl":"","userId":"17593568044731677741"}}},"source":["len(selected_words)"],"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7875005"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"code","metadata":{"id":"SbKPwrD931GI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"abd3e959-4ad1-4df4-c88c-a2b4ff2f1a81","executionInfo":{"status":"ok","timestamp":1586169580755,"user_tz":-180,"elapsed":51243,"user":{"displayName":"isaac truitt","photoUrl":"","userId":"17593568044731677741"}}},"source":["%%time\n","\n","inference_sents = [text_to_ints(word, vocab_to_int) for word in selected_words]"],"execution_count":36,"outputs":[{"output_type":"stream","text":["CPU times: user 23 s, sys: 238 ms, total: 23.2 s\n","Wall time: 23.2 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YQytojDzqxVm","colab_type":"code","colab":{}},"source":["del selected_words"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xYmX9rMo31Uj","colab_type":"code","colab":{}},"source":["checkpoint = \"/content/gdrive/My Drive/RLC/best_obfuscator2/best_obfuscator.ckpt\"\n","\n","model = build_graph(keep_probability, rnn_size, num_layers, batch_size, learning_rate, embedding_size, direction) "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hFndCM9gffCM","colab_type":"code","outputId":"82e53c88-923b-420b-ac6b-2d0b3f86d4be","executionInfo":{"status":"ok","timestamp":1586173818317,"user_tz":-180,"elapsed":2290449,"user":{"displayName":"isaac truitt","photoUrl":"","userId":"17593568044731677741"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["%%time\n","\n","results = []\n","\n","with tf.Session() as sess:\n","    # Load saved model\n","    saver = tf.train.Saver()\n","    saver.restore(sess, checkpoint)\n","    \n","    for input_batch, input_length in get_batches(inference_sents, batch_size):\n","        answer_logits = sess.run(model.predictions, {model.inputs: input_batch, \n","                                                    model.inputs_length: input_length,\n","                                                    model.targets_length: [max(input_length)+1], \n","                                                    model.keep_prob: [1.0]})\n","        results += list(answer_logits)"],"execution_count":42,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/RLC/best_obfuscator2/best_obfuscator.ckpt\n","CPU times: user 2h 2min 9s, sys: 21min 3s, total: 2h 23min 12s\n","Wall time: 1h 9min 53s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Dgv3N0s325dt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"dd6a3b6d-5051-4d16-cee5-6a7e2cd0cb83","executionInfo":{"status":"ok","timestamp":1586173864053,"user_tz":-180,"elapsed":45751,"user":{"displayName":"isaac truitt","photoUrl":"","userId":"17593568044731677741"}}},"source":["%%time\n","\n","#del inference_sents\n","\n","results_decoded = [\"\".join([int_to_vocab[int(i)] for i in answer]) for answer in results]\n","results_decoded = [res[:res.find(\"<EOS>\")] for res in results_decoded]"],"execution_count":43,"outputs":[{"output_type":"stream","text":["CPU times: user 45.6 s, sys: 0 ns, total: 45.6 s\n","Wall time: 45.6 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EWyhIVoMpUmX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":193},"outputId":"3758b8c9-4e2b-4849-8804-9a8e8e8b9545","executionInfo":{"status":"ok","timestamp":1586173864054,"user_tz":-180,"elapsed":52,"user":{"displayName":"isaac truitt","photoUrl":"","userId":"17593568044731677741"}}},"source":["results_decoded[:10]"],"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['архив',\n"," 'ряда',\n"," 'контролью',\n"," 'средства',\n"," 'по',\n"," 'секторое',\n"," 'улицы',\n"," 'деятельностия',\n"," 'км',\n"," 'через']"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"code","metadata":{"id":"zu1B75n08o6C","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":193},"outputId":"390d0964-0534-47e6-c4b0-dcad5e7f1ab1","executionInfo":{"status":"ok","timestamp":1586173864056,"user_tz":-180,"elapsed":32,"user":{"displayName":"isaac truitt","photoUrl":"","userId":"17593568044731677741"}}},"source":["results_decoded[110:120]"],"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['наша',\n"," 'отмечает',\n"," 'государственный',\n"," 'или',\n"," 'принятия',\n"," 'декларации',\n"," 'являлься',\n"," 'мет',\n"," 'предуссмотрен',\n"," 'походы']"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"code","metadata":{"id":"5jnSoDXF8ymD","colab_type":"code","colab":{}},"source":["import pickle\n","\n","with open(\"res2.pickle\", \"wb\") as outp:\n","  pickle.dump(results_decoded, outp)\n","\n","!cp res2.pickle /content/gdrive/My\\ Drive/RLC2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iMLS3CbaBwX7","colab_type":"code","colab":{}},"source":["import pickle\n","\n","with open(\"/content/gdrive/My Drive/RLC2/res2.pickle\", \"rb\") as inp:\n","  results_decoded = pickle.load(inp)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HgYK3Tf7nRLa","colab_type":"code","colab":{}},"source":["#del results\n","\n","leipzig_df = pd.read_pickle(\"/content/gdrive/My Drive/RLC2/leipzig_df_we.pickle\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RNISUQmn_nPP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":639},"outputId":"4d8a6b70-e83d-4731-8ab4-d79c012df24e","executionInfo":{"status":"ok","timestamp":1586173936781,"user_tz":-180,"elapsed":25646,"user":{"displayName":"isaac truitt","photoUrl":"","userId":"17593568044731677741"}}},"source":["rd = iter(results_decoded)\n","errs = list(leipzig_df[\"error\"])\n","\n","found_t = 0\n","for i, e in enumerate(errs):\n","  if e:\n","    found_t += 1\n","  if found_t >= len(results_decoded):\n","    errs[i] = False\n","\n","errors1 = [next(rd) if e else None for e in errs]\n","leipzig_df[\"errmodel2\"] = errors1\n","\n","leipzig_df.sample(20)"],"execution_count":49,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sent_id</th>\n","      <th>word</th>\n","      <th>whitespaces_follow</th>\n","      <th>error</th>\n","      <th>nones</th>\n","      <th>errmodel1</th>\n","      <th>errmodel2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>38604266</th>\n","      <td>wikipedia_227347</td>\n","      <td>лет</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>23210230</th>\n","      <td>webpublic_327690</td>\n","      <td>,</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>43439554</th>\n","      <td>wikipedia_492668</td>\n","      <td>без</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>29114858</th>\n","      <td>webpublic_680648</td>\n","      <td>1943</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>13302018</th>\n","      <td>newscrawl_750951</td>\n","      <td>Проголосовать</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>7403736</th>\n","      <td>newscrawl_414570</td>\n","      <td>рыбе</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>37289882</th>\n","      <td>wikipedia_157187</td>\n","      <td>Опыт</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>6073253</th>\n","      <td>newscrawl_339833</td>\n","      <td>состоянию</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>12449306</th>\n","      <td>newscrawl_703016</td>\n","      <td>планирует</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>38934447</th>\n","      <td>wikipedia_245424</td>\n","      <td>до</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>39678474</th>\n","      <td>wikipedia_285226</td>\n","      <td>законченности</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>10194311</th>\n","      <td>newscrawl_576171</td>\n","      <td>дней</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>34437007</th>\n","      <td>wikipedia_354</td>\n","      <td>марта</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>31972322</th>\n","      <td>webpublic_848269</td>\n","      <td>которым</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>которым</td>\n","      <td>которым</td>\n","    </tr>\n","    <tr>\n","      <th>13527028</th>\n","      <td>newscrawl_764506</td>\n","      <td>государство</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>38407277</th>\n","      <td>wikipedia_216789</td>\n","      <td>«</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>8289260</th>\n","      <td>newscrawl_466261</td>\n","      <td>Обаме</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>27323852</th>\n","      <td>webpublic_574986</td>\n","      <td>,</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>48623345</th>\n","      <td>wikipedia_782113</td>\n","      <td>году</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>45926740</th>\n","      <td>wikipedia_633069</td>\n","      <td>х</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                   sent_id           word  ...  errmodel1  errmodel2\n","38604266  wikipedia_227347            лет  ...       None       None\n","23210230  webpublic_327690              ,  ...       None       None\n","43439554  wikipedia_492668            без  ...       None       None\n","29114858  webpublic_680648           1943  ...       None       None\n","13302018  newscrawl_750951  Проголосовать  ...       None       None\n","7403736   newscrawl_414570           рыбе  ...       None       None\n","37289882  wikipedia_157187           Опыт  ...       None       None\n","6073253   newscrawl_339833      состоянию  ...       None       None\n","12449306  newscrawl_703016      планирует  ...       None       None\n","38934447  wikipedia_245424             до  ...       None       None\n","39678474  wikipedia_285226  законченности  ...       None       None\n","10194311  newscrawl_576171           дней  ...       None       None\n","34437007     wikipedia_354          марта  ...       None       None\n","31972322  webpublic_848269        которым  ...    которым    которым\n","13527028  newscrawl_764506    государство  ...       None       None\n","38407277  wikipedia_216789              «  ...       None       None\n","8289260   newscrawl_466261          Обаме  ...       None       None\n","27323852  webpublic_574986              ,  ...       None       None\n","48623345  wikipedia_782113           году  ...       None       None\n","45926740  wikipedia_633069              х  ...       None       None\n","\n","[20 rows x 7 columns]"]},"metadata":{"tags":[]},"execution_count":49}]},{"cell_type":"code","metadata":{"id":"34jpnA6oCU49","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":639},"outputId":"f071bb55-86b2-4918-8483-e365a6e00cac","executionInfo":{"status":"ok","timestamp":1586173940133,"user_tz":-180,"elapsed":3366,"user":{"displayName":"isaac truitt","photoUrl":"","userId":"17593568044731677741"}}},"source":["leipzig_df.sample(20)"],"execution_count":50,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sent_id</th>\n","      <th>word</th>\n","      <th>whitespaces_follow</th>\n","      <th>error</th>\n","      <th>nones</th>\n","      <th>errmodel1</th>\n","      <th>errmodel2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>36961811</th>\n","      <td>wikipedia_139018</td>\n","      <td>созданию</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>3328729</th>\n","      <td>newscrawl_185230</td>\n","      <td>%</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>13787417</th>\n","      <td>newscrawl_779590</td>\n","      <td>справляется</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>2194404</th>\n","      <td>newscrawl_123113</td>\n","      <td>проведения</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>проведия</td>\n","      <td>проведения</td>\n","    </tr>\n","    <tr>\n","      <th>10700746</th>\n","      <td>newscrawl_606281</td>\n","      <td>два</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>3108519</th>\n","      <td>newscrawl_173616</td>\n","      <td>В</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>14171995</th>\n","      <td>newscrawl_800922</td>\n","      <td>заместителем</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>заметилем</td>\n","      <td>заместителем</td>\n","    </tr>\n","    <tr>\n","      <th>47747262</th>\n","      <td>wikipedia_733234</td>\n","      <td>нас</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>32343153</th>\n","      <td>webpublic_870315</td>\n","      <td>примерно</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>примено</td>\n","      <td>примерно</td>\n","    </tr>\n","    <tr>\n","      <th>35999244</th>\n","      <td>wikipedia_85760</td>\n","      <td>В</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>716795</th>\n","      <td>newscrawl_41891</td>\n","      <td>“</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>49238442</th>\n","      <td>wikipedia_817548</td>\n","      <td>тем</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>2006841</th>\n","      <td>newscrawl_113047</td>\n","      <td>»,</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>42847246</th>\n","      <td>wikipedia_459112</td>\n","      <td>транслировалось</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>39237221</th>\n","      <td>wikipedia_261194</td>\n","      <td>филармонии</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>38605948</th>\n","      <td>wikipedia_227439</td>\n","      <td>посчитал</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>23390574</th>\n","      <td>webpublic_338794</td>\n","      <td>И</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>20559292</th>\n","      <td>webpublic_171855</td>\n","      <td>)</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>16779665</th>\n","      <td>newscrawl_951118</td>\n","      <td>с</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>40401885</th>\n","      <td>wikipedia_325879</td>\n","      <td>плохо</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                   sent_id             word  ...  errmodel1     errmodel2\n","36961811  wikipedia_139018         созданию  ...       None          None\n","3328729   newscrawl_185230                %  ...       None          None\n","13787417  newscrawl_779590      справляется  ...       None          None\n","2194404   newscrawl_123113       проведения  ...   проведия    проведения\n","10700746  newscrawl_606281              два  ...       None          None\n","3108519   newscrawl_173616                В  ...       None          None\n","14171995  newscrawl_800922     заместителем  ...  заметилем  заместителем\n","47747262  wikipedia_733234              нас  ...       None          None\n","32343153  webpublic_870315         примерно  ...    примено      примерно\n","35999244   wikipedia_85760                В  ...       None          None\n","716795     newscrawl_41891                “  ...       None          None\n","49238442  wikipedia_817548              тем  ...       None          None\n","2006841   newscrawl_113047               »,  ...       None          None\n","42847246  wikipedia_459112  транслировалось  ...       None          None\n","39237221  wikipedia_261194       филармонии  ...       None          None\n","38605948  wikipedia_227439         посчитал  ...       None          None\n","23390574  webpublic_338794                И  ...       None          None\n","20559292  webpublic_171855                )  ...       None          None\n","16779665  newscrawl_951118                с  ...       None          None\n","40401885  wikipedia_325879            плохо  ...       None          None\n","\n","[20 rows x 7 columns]"]},"metadata":{"tags":[]},"execution_count":50}]},{"cell_type":"code","metadata":{"id":"C-X_2vHZo2Mb","colab_type":"code","colab":{}},"source":["leipzig_df.to_pickle(\"leipzig_df_we.pickle\")\n","!cp leipzig_df_we.pickle /content/gdrive/My\\ Drive/RLC2"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"Nqr1e4Jnb_Xv","colab_type":"text"},"source":["Examples of corrected sentences:\n","- Spellin is difficult, whch is wyh you need to study everyday.\n","- Spelling is difficult, which is why you need to study everyday.\n","\n","\n","- The first days of her existence in th country were vrey hard for Dolly. \n","- The first days of her existence in the country were very hard for Dolly.\n","\n","\n","- Thi is really something impressiv thaat we should look into right away! \n","- This is really something impressive that we should look into right away!"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"RBGrBPBSb_Xx","colab_type":"text"},"source":["## Summary"]},{"cell_type":"markdown","metadata":{"id":"v4pp2FOhb_X0","colab_type":"text"},"source":["I hope that you have found this project to be rather interesting and useful. The example sentences that I have presented above were specifically chosen, and the model will not always be able to make corrections of this quality. Given the amount of data that we are working with, this model still struggles. For it to be more useful, it would require far more training data, and additional parameter tuning. This parameter values that I have above worked best for me, but I expect there are even better values that I was not able to find.\n","\n","Thanks for reading!"]}]}