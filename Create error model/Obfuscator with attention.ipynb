{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"},"colab":{"name":"Obfuscator with attention","provenance":[{"file_id":"https://github.com/Currie32/Spell-Checker/blob/master/SpellChecker.ipynb","timestamp":1585903674868}],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Vk9XlE2vb_Se","colab_type":"text"},"source":["# Creating a Spell Checker"]},{"cell_type":"markdown","metadata":{"id":"Newnp48xb_Sl","colab_type":"text"},"source":["The objective of this project is to build a model that can take a sentence with spelling mistakes as input, and output the same sentence, but with the mistakes corrected. The data that we will use for this project will be twenty popular books from [Project Gutenberg](http://www.gutenberg.org/ebooks/search/?sort_order=downloads). Our model is designed using grid search to find the optimal architecture, and hyperparameter values. The best results, as measured by sequence loss with 15% of our data, were created using a two-layered network with a bi-direction RNN in the encoding layer and Bahdanau Attention in the decoding layer. [FloydHub's](https://www.floydhub.com/) GPU service was used to train the model.\n","\n","The sections of the project are:\n","- Loading the Data\n","- Preparing the Data\n","- Building the Model\n","- Training the Model\n","- Fixing Custom Sentences\n","- Summary"]},{"cell_type":"code","metadata":{"id":"0DYQYdyBaPzD","colab_type":"code","colab":{}},"source":["learn_noise = True #@param {type:\"boolean\"}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"krWE6guj2QzA","colab_type":"code","outputId":"b3ac1450-4b6f-4492-d4d0-5817f1fe4ddd","executionInfo":{"status":"ok","timestamp":1585932251146,"user_tz":-180,"elapsed":1611,"user":{"displayName":"Ivan Torubarov","photoUrl":"","userId":"07481003931234110333"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%tensorflow_version 1.x"],"execution_count":2,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Mw4ulm8_1sbv","colab_type":"code","outputId":"7483ad5d-1107-4617-f0fc-d40e15b865ed","executionInfo":{"status":"ok","timestamp":1585932251148,"user_tz":-180,"elapsed":1598,"user":{"displayName":"Ivan Torubarov","photoUrl":"","userId":"07481003931234110333"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\")"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ur_W654Vb_Ss","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","import os\n","from copy import deepcopy\n","from os import listdir\n","from os.path import isfile, join\n","from collections import namedtuple\n","from tensorflow.python.layers.core import Dense\n","from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n","import time\n","import re\n","from sklearn.model_selection import train_test_split"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c12JxAJtb_S5","colab_type":"text"},"source":["## Loading the Data"]},{"cell_type":"code","metadata":{"id":"L48BweVVO1Ki","colab_type":"code","colab":{}},"source":["spelling_errs = pd.read_pickle(\"/content/gdrive/My Drive/RLC/spelling_errs.pickle\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ojgmVq8BZRhZ","colab_type":"code","colab":{}},"source":["test_frac = 0.1\n","\n","spelling_errs = spelling_errs.sample(frac=1, random_state=42).reset_index(drop=True)\n","test_df = spelling_errs[:int(len(spelling_errs) * test_frac)]\n","spelling_errs = spelling_errs[int(len(spelling_errs) * test_frac):]\n","\n","test_df.to_pickle(\"test.pickle\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3_mEQCKiZwiM","colab_type":"code","colab":{}},"source":["if learn_noise:\n","  !cp test.pickle /content/gdrive/My\\ Drive/RLC/obfuscator_test.pickle\n","else:\n","  !cp test.pickle /content/gdrive/My\\ Drive/RLC/corrector_test.pickle"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"m02xOpK-15G_","colab_type":"code","colab":{}},"source":["errors = [word.lower() for word in list(spelling_errs[\"Ошибка\"])]\n","corrections = [word.lower() for word in list(spelling_errs[\"Исправление\"])]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"O9wnSpMmPrMh","colab_type":"code","outputId":"a040e39f-a2d6-4437-c201-fbdad6af0a28","executionInfo":{"status":"ok","timestamp":1585932256151,"user_tz":-180,"elapsed":4833,"user":{"displayName":"Ivan Torubarov","photoUrl":"","userId":"07481003931234110333"}},"colab":{"base_uri":"https://localhost:8080/","height":193}},"source":["for p in zip(errors[:10], corrections[:10]):\n","  print(p[0], p[1], sep=\"\\t\")"],"execution_count":9,"outputs":[{"output_type":"stream","text":["штейн\tстайн\n","не   многие\tнемногие\n","енергии\tэнергии\n","беспокоит\tбеспокоить\n","развывающиеся\tразвивающиеся\n","не однородный\tнеоднородный\n","левой\tлевые\n","епигона\tэпигона\n","элизавет\tэлизабет\n","науке против старения\tнауке против старения.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yQQl9aEgb_Te","colab_type":"text"},"source":["## Preparing the Data"]},{"cell_type":"code","metadata":{"id":"7uEnZw0jb_Tg","colab_type":"code","colab":{}},"source":["def clean_text(text):\n","    '''Remove unwanted characters and extra spaces from the text'''\n","    return text.lower()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JLNAx0fTb_Tt","colab_type":"code","colab":{}},"source":["# Clean the text of the books\n","clean_books = []\n","for p in zip(errors, corrections):\n","  if learn_noise:\n","    clean_books.append([clean_text(p[1]), clean_text(p[0])])\n","  else:\n","    clean_books.append([clean_text(p[0]), clean_text(p[1])])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tlgQSfAcb_US","colab_type":"code","colab":{}},"source":["# Create a dictionary to convert the vocabulary (characters) to integers\n","vocab_to_int = {}\n","count = 0\n","for book in clean_books:\n","    for character in book[0]+book[1]:\n","        if character not in vocab_to_int:\n","            vocab_to_int[character] = count\n","            count += 1\n","\n","# Add special tokens to vocab_to_int\n","codes = ['<PAD>','<EOS>','<GO>']\n","for code in codes:\n","    vocab_to_int[code] = count\n","    count += 1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5kIw8DXdb_Uh","colab_type":"code","outputId":"965191e5-6d11-47d1-d8e2-5e91a7518d9d","executionInfo":{"status":"ok","timestamp":1585932256155,"user_tz":-180,"elapsed":3745,"user":{"displayName":"Ivan Torubarov","photoUrl":"","userId":"07481003931234110333"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["# Check the size of vocabulary and all of the values\n","vocab_size = len(vocab_to_int)\n","print(\"The vocabulary contains {} characters.\".format(vocab_size))\n","print(sorted(vocab_to_int))"],"execution_count":13,"outputs":[{"output_type":"stream","text":["The vocabulary contains 98 characters.\n","[' ', '!', '\"', '#', '%', '&', \"'\", '(', ')', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '<EOS>', '<GO>', '<PAD>', '>', '?', 'a', 'b', 'c', 'd', 'e', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\xa0', '«', '»', 'è', 'ë', '́', 'а', 'б', 'в', 'г', 'д', 'е', 'ж', 'з', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', 'я', 'ё', 'қ', '–', '—', '’', '№']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"30GvI1Elb_Uq","colab_type":"text"},"source":["*Note: We could have made this project a little easier by using only lower case words and fewer special characters ($,&,-...), but I want to make this spell checker as useful as possible.*"]},{"cell_type":"code","metadata":{"id":"PzBpQntTb_Uu","colab_type":"code","colab":{}},"source":["# Create another dictionary to convert integers to their respective characters\n","int_to_vocab = {}\n","for character, value in vocab_to_int.items():\n","    int_to_vocab[value] = character"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"g8l2TDjJb_U5","colab_type":"code","outputId":"995d1adc-0427-4dbc-b00a-a1236e44b179","executionInfo":{"status":"ok","timestamp":1585932256157,"user_tz":-180,"elapsed":3130,"user":{"displayName":"Ivan Torubarov","photoUrl":"","userId":"07481003931234110333"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Split the text from the books into sentences.\n","sentences = deepcopy(clean_books)\n","print(\"There are {} sentences.\".format(len(sentences)))"],"execution_count":15,"outputs":[{"output_type":"stream","text":["There are 8470 sentences.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5j2F2ysNb_VE","colab_type":"code","colab":{}},"source":["# Convert sentences to integers\n","int_sentences = []\n","\n","for sentence in sentences:\n","    int_sentence = []\n","    for character in sentence[0]:\n","        int_sentence.append(vocab_to_int[character])\n","    int_sentences.append(int_sentence)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sDicsPG7b_VJ","colab_type":"code","colab":{}},"source":["# Find the length of each sentence\n","lengths = []\n","for sentence in int_sentences:\n","    lengths.append(len(sentence))\n","lengths = pd.DataFrame(lengths, columns=[\"counts\"])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jyMHg-nYb_VS","colab_type":"code","outputId":"69e2558d-9a39-4388-ae42-68bffbf4158f","executionInfo":{"status":"ok","timestamp":1585932256158,"user_tz":-180,"elapsed":2259,"user":{"displayName":"Ivan Torubarov","photoUrl":"","userId":"07481003931234110333"}},"colab":{"base_uri":"https://localhost:8080/","height":284}},"source":["lengths.describe()"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>counts</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>8470.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>9.057497</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>4.644462</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>7.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>8.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>11.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>142.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            counts\n","count  8470.000000\n","mean      9.057497\n","std       4.644462\n","min       1.000000\n","25%       7.000000\n","50%       8.000000\n","75%      11.000000\n","max     142.000000"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"bpr01oT7b_VZ","colab_type":"code","outputId":"a88ad629-6dad-42ea-f839-45f717bbafd4","executionInfo":{"status":"ok","timestamp":1585932256159,"user_tz":-180,"elapsed":1997,"user":{"displayName":"Ivan Torubarov","photoUrl":"","userId":"07481003931234110333"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Limit the data we will use to train our model\n","max_length = int(lengths.describe().loc[\"75%\"])+1\n","min_length = 2\n","\n","good_sentences = []\n","\n","for sentence in int_sentences:\n","    if len(sentence) <= max_length and len(sentence) >= min_length:\n","        good_sentences.append(sentence)\n","\n","print(\"We will use {} to train and test our model.\".format(len(good_sentences)))"],"execution_count":19,"outputs":[{"output_type":"stream","text":["We will use 7383 to train and test our model.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Y4Y8Hx8eb_Vf","colab_type":"text"},"source":["*Note: I decided to not use very long or short sentences because they are not as useful for training our model. Shorter sentences are less likely to include an error and the text is more likely to be repetitive. Longer sentences are more difficult to learn due to their length and increase the training time quite a bit. If you are interested in using this model for more than just a personal project, it would be worth using these longer sentence, and much more training data to create a more accurate model.*"]},{"cell_type":"code","metadata":{"id":"t2XdN-1cb_Vh","colab_type":"code","outputId":"ab3fdef7-faa7-4610-bbd9-cc16fd72595d","executionInfo":{"status":"ok","timestamp":1585932256159,"user_tz":-180,"elapsed":1728,"user":{"displayName":"Ivan Torubarov","photoUrl":"","userId":"07481003931234110333"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["# Split the data into training and testing sentences\n","training, testing = train_test_split(good_sentences, test_size = 0.15, random_state = 42)\n","\n","print(\"Number of training sentences:\", len(training))\n","print(\"Number of testing sentences:\", len(testing))"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Number of training sentences: 6275\n","Number of testing sentences: 1108\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wJdcHaw5b_Vo","colab_type":"code","colab":{}},"source":["# Sort the sentences by length to reduce padding, which will allow the model to train faster\n","training_sorted = []\n","testing_sorted = []\n","\n","for i in range(min_length, max_length+1):\n","    for sentence in training:\n","        if len(sentence) == i:\n","            training_sorted.append(sentence)\n","    for sentence in testing:\n","        if len(sentence) == i:\n","            testing_sorted.append(sentence)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PAPRoXsyb_Vw","colab_type":"code","outputId":"9cde6221-ecc0-4080-dd93-9d8a8fe4ae7b","executionInfo":{"status":"ok","timestamp":1585932256160,"user_tz":-180,"elapsed":1110,"user":{"displayName":"Ivan Torubarov","photoUrl":"","userId":"07481003931234110333"}},"colab":{"base_uri":"https://localhost:8080/","height":105}},"source":["# Check to ensure the sentences have been selected and sorted correctly\n","for i in range(5):\n","    print(training_sorted[i], len(training_sorted[i]))"],"execution_count":22,"outputs":[{"output_type":"stream","text":["[4, 8] 2\n","[10, 18] 2\n","[6, 41] 2\n","[4, 2] 2\n","[18, 2] 2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"y5XaefnyEQEf","colab_type":"code","colab":{}},"source":["def decoding_seq(int_sequence):\n","  return \"\".join([int_to_vocab[i] for i in int_sequence])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0oavT5LIb_V9","colab_type":"code","colab":{}},"source":["inps = [p[0] for p in clean_books]\n","noise = [p[1] for p in clean_books]\n","\n","training_noisy = []\n","for seq in training_sorted:\n","  training_noisy.append([vocab_to_int[c] for c in noise[inps.index(decoding_seq(seq))]])\n","\n","testing_noisy = []\n","for seq in testing_sorted:\n","  testing_noisy.append([vocab_to_int[c] for c in noise[inps.index(decoding_seq(seq))]])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"5OhavmMcb_WO","colab_type":"text"},"source":["# Building the Model"]},{"cell_type":"code","metadata":{"id":"0yc7VJC0b_WQ","colab_type":"code","colab":{}},"source":["def model_inputs():\n","    '''Create palceholders for inputs to the model'''\n","    \n","    with tf.name_scope('inputs'):\n","        inputs = tf.placeholder(tf.int32, [None, None], name='inputs')\n","    with tf.name_scope('targets'):\n","        targets = tf.placeholder(tf.int32, [None, None], name='targets')\n","    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n","    inputs_length = tf.placeholder(tf.int32, (None,), name='inputs_length')\n","    targets_length = tf.placeholder(tf.int32, (None,), name='targets_length')\n","    max_target_length = tf.reduce_max(targets_length, name='max_target_len')\n","\n","    return inputs, targets, keep_prob, inputs_length, targets_length, max_target_length"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"g5W49COfb_Wd","colab_type":"code","colab":{}},"source":["def process_encoding_input(targets, vocab_to_int, batch_size):\n","    '''Remove the last word id from each batch and concat the <GO> to the begining of each batch'''\n","    \n","    with tf.name_scope(\"process_encoding\"):\n","        ending = tf.strided_slice(targets, [0, 0], [batch_size, -1], [1, 1])\n","        dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n","\n","    return dec_input"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EYTg52tFb_Wk","colab_type":"code","colab":{}},"source":["def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob, direction):\n","    '''Create the encoding layer'''\n","    \n","    if direction == 1:\n","        with tf.name_scope(\"RNN_Encoder_Cell_1D\"):\n","            for layer in range(num_layers):\n","                with tf.variable_scope('encoder_{}'.format(layer)):\n","                    lstm = tf.contrib.rnn.LSTMCell(rnn_size)\n","\n","                    drop = tf.contrib.rnn.DropoutWrapper(lstm, \n","                                                         input_keep_prob = keep_prob)\n","\n","                    enc_output, enc_state = tf.nn.dynamic_rnn(drop, \n","                                                              rnn_inputs,\n","                                                              sequence_length,\n","                                                              dtype=tf.float32)\n","\n","            return enc_output, enc_state\n","        \n","        \n","    if direction == 2:\n","        with tf.name_scope(\"RNN_Encoder_Cell_2D\"):\n","            for layer in range(num_layers):\n","                with tf.variable_scope('encoder_{}'.format(layer)):\n","                    cell_fw = tf.contrib.rnn.LSTMCell(rnn_size)\n","                    cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \n","                                                            input_keep_prob = keep_prob)\n","\n","                    cell_bw = tf.contrib.rnn.LSTMCell(rnn_size)\n","                    cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \n","                                                            input_keep_prob = keep_prob)\n","\n","                    enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n","                                                                            cell_bw, \n","                                                                            rnn_inputs,\n","                                                                            sequence_length,\n","                                                                            dtype=tf.float32)\n","            # Join outputs since we are using a bidirectional RNN\n","            enc_output = tf.concat(enc_output,2)\n","            # Use only the forward state because the model can't use both states at once\n","            return enc_output, enc_state[0]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OOPkZ568b_Wr","colab_type":"code","colab":{}},"source":["def training_decoding_layer(dec_embed_input, targets_length, dec_cell, initial_state, output_layer, \n","                            vocab_size, max_target_length):\n","    '''Create the training logits'''\n","    \n","    with tf.name_scope(\"Training_Decoder\"):\n","        training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n","                                                            sequence_length=targets_length,\n","                                                            time_major=False)\n","\n","        training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n","                                                           training_helper,\n","                                                           initial_state,\n","                                                           output_layer) \n","\n","        training_logits, _ ,_ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n","                                                                    output_time_major=False,\n","                                                                    impute_finished=True,\n","                                                                    maximum_iterations=max_target_length)\n","        return training_logits"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ifuDRcyrb_Wv","colab_type":"code","colab":{}},"source":["def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, initial_state, output_layer,\n","                             max_target_length, batch_size):\n","    '''Create the inference logits'''\n","    \n","    with tf.name_scope(\"Inference_Decoder\"):\n","        start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n","\n","        inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n","                                                                    start_tokens,\n","                                                                    end_token)\n","\n","        inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n","                                                            inference_helper,\n","                                                            initial_state,\n","                                                            output_layer)\n","\n","        inference_logits, _ ,_ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n","                                                                    output_time_major=False,\n","                                                                    impute_finished=True,\n","                                                                    maximum_iterations=max_target_length)\n","\n","        return inference_logits"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jAADlTiUb_W2","colab_type":"code","colab":{}},"source":["def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, inputs_length, targets_length, \n","                   max_target_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers, direction):\n","    '''Create the decoding cell and attention for the training and inference decoding layers'''\n","    \n","    with tf.name_scope(\"RNN_Decoder_Cell\"):\n","        for layer in range(num_layers):\n","            with tf.variable_scope('decoder_{}'.format(layer)):\n","                lstm = tf.contrib.rnn.LSTMCell(rnn_size)\n","                dec_cell = tf.contrib.rnn.DropoutWrapper(lstm, \n","                                                         input_keep_prob = keep_prob)\n","    \n","    output_layer = Dense(vocab_size,\n","                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n","    \n","    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n","                                                  enc_output,\n","                                                  inputs_length,\n","                                                  normalize=False,\n","                                                  name='BahdanauAttention')\n","    \n","    with tf.name_scope(\"Attention_Wrapper\"):\n","        dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell,\n","                                                      attn_mech,\n","                                                      rnn_size)\n","    \n","    initial_state = dec_cell.zero_state(batch_size=batch_size,dtype=tf.float32).clone(cell_state=enc_state)\n","\n","    with tf.variable_scope(\"decode\"):\n","        training_logits = training_decoding_layer(dec_embed_input, \n","                                                  targets_length, \n","                                                  dec_cell, \n","                                                  initial_state,\n","                                                  output_layer,\n","                                                  vocab_size, \n","                                                  max_target_length)\n","    with tf.variable_scope(\"decode\", reuse=True):\n","        inference_logits = inference_decoding_layer(embeddings,  \n","                                                    vocab_to_int['<GO>'], \n","                                                    vocab_to_int['<EOS>'],\n","                                                    dec_cell, \n","                                                    initial_state, \n","                                                    output_layer,\n","                                                    max_target_length,\n","                                                    batch_size)\n","\n","    return training_logits, inference_logits"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GJX_hK8ob_W6","colab_type":"code","colab":{}},"source":["def seq2seq_model(inputs, targets, keep_prob, inputs_length, targets_length, max_target_length, \n","                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size, embedding_size, direction):\n","    '''Use the previous functions to create the training and inference logits'''\n","    \n","    enc_embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1, 1))\n","    enc_embed_input = tf.nn.embedding_lookup(enc_embeddings, inputs)\n","    enc_output, enc_state = encoding_layer(rnn_size, inputs_length, num_layers, \n","                                           enc_embed_input, keep_prob, direction)\n","    \n","    dec_embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1, 1))\n","    dec_input = process_encoding_input(targets, vocab_to_int, batch_size)\n","    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n","    \n","    training_logits, inference_logits  = decoding_layer(dec_embed_input, \n","                                                        dec_embeddings,\n","                                                        enc_output,\n","                                                        enc_state, \n","                                                        vocab_size, \n","                                                        inputs_length, \n","                                                        targets_length, \n","                                                        max_target_length,\n","                                                        rnn_size, \n","                                                        vocab_to_int, \n","                                                        keep_prob, \n","                                                        batch_size,\n","                                                        num_layers,\n","                                                        direction)\n","    \n","    return training_logits, inference_logits"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"P9k9z9ztb_W-","colab_type":"code","colab":{}},"source":["def pad_sentence_batch(sentence_batch):\n","    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n","    max_sentence = max([len(sentence) for sentence in sentence_batch])\n","    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VWs5OUWHb_XD","colab_type":"code","colab":{}},"source":["def get_batches(sentences, noisy_sentences, batch_size, threshold):\n","    \"\"\"Batch sentences, noisy sentences, and the lengths of their sentences together.\n","       With each epoch, sentences will receive new mistakes\"\"\"\n","    \n","    for batch_i in range(0, len(sentences)//batch_size):\n","        start_i = batch_i * batch_size\n","        sentences_batch = sentences[start_i:start_i + batch_size]\n","        \n","        sentences_batch_noisy = noisy_sentences[start_i:start_i + batch_size]\n","            \n","        sentences_batch_eos = []\n","        for sentence in sentences_batch:\n","            sentence.append(vocab_to_int['<EOS>'])\n","            sentences_batch_eos.append(sentence)\n","            \n","        pad_sentences_batch = np.array(pad_sentence_batch(sentences_batch_eos))\n","        pad_sentences_noisy_batch = np.array(pad_sentence_batch(sentences_batch_noisy))\n","        \n","        # Need the lengths for the _lengths parameters\n","        pad_sentences_lengths = []\n","        for sentence in pad_sentences_batch:\n","            pad_sentences_lengths.append(len(sentence))\n","        \n","        pad_sentences_noisy_lengths = []\n","        for sentence in pad_sentences_noisy_batch:\n","            pad_sentences_noisy_lengths.append(len(sentence))\n","        \n","        yield pad_sentences_noisy_batch, pad_sentences_batch, pad_sentences_noisy_lengths, pad_sentences_lengths"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PgnP4Nlub_XK","colab_type":"text"},"source":["*Note: This set of values achieved the best results.*"]},{"cell_type":"code","metadata":{"id":"xGxUa8_eb_XK","colab_type":"code","colab":{}},"source":["# The default parameters\n","epochs = 100\n","batch_size = 128\n","num_layers = 2\n","rnn_size = 512\n","embedding_size = 128\n","learning_rate = 0.0005\n","direction = 2\n","threshold = 0.95\n","keep_probability = 0.75"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"w2sLe5tCb_XS","colab_type":"code","colab":{}},"source":["def build_graph(keep_prob, rnn_size, num_layers, batch_size, learning_rate, embedding_size, direction):\n","\n","    tf.reset_default_graph()\n","    \n","    # Load the model inputs    \n","    inputs, targets, keep_prob, inputs_length, targets_length, max_target_length = model_inputs()\n","\n","    # Create the training and inference logits\n","    training_logits, inference_logits = seq2seq_model(tf.reverse(inputs, [-1]),\n","                                                      targets, \n","                                                      keep_prob,   \n","                                                      inputs_length,\n","                                                      targets_length,\n","                                                      max_target_length,\n","                                                      len(vocab_to_int)+1,\n","                                                      rnn_size, \n","                                                      num_layers, \n","                                                      vocab_to_int,\n","                                                      batch_size,\n","                                                      embedding_size,\n","                                                      direction)\n","\n","    # Create tensors for the training logits and inference logits\n","    training_logits = tf.identity(training_logits.rnn_output, 'logits')\n","\n","    with tf.name_scope('predictions'):\n","        predictions = tf.identity(inference_logits.sample_id, name='predictions')\n","        tf.summary.histogram('predictions', predictions)\n","\n","    # Create the weights for sequence_loss\n","    masks = tf.sequence_mask(targets_length, max_target_length, dtype=tf.float32, name='masks')\n","    \n","    with tf.name_scope(\"cost\"):\n","        # Loss function\n","        cost = tf.contrib.seq2seq.sequence_loss(training_logits, \n","                                                targets, \n","                                                masks)\n","        tf.summary.scalar('cost', cost)\n","\n","    with tf.name_scope(\"optimze\"):\n","        optimizer = tf.train.AdamOptimizer(learning_rate)\n","\n","        # Gradient Clipping\n","        gradients = optimizer.compute_gradients(cost)\n","        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n","        train_op = optimizer.apply_gradients(capped_gradients)\n","\n","    # Merge all of the summaries\n","    merged = tf.summary.merge_all()    \n","\n","    # Export the nodes \n","    export_nodes = ['inputs', 'targets', 'keep_prob', 'cost', 'inputs_length', 'targets_length',\n","                    'predictions', 'merged', 'train_op','optimizer']\n","    Graph = namedtuple('Graph', export_nodes)\n","    local_dict = locals()\n","    graph = Graph(*[local_dict[each] for each in export_nodes])\n","\n","    return graph"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"4FwpPYK5b_XZ","colab_type":"text"},"source":["## Training the Model"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"EM5onhdNb_Xa","colab_type":"code","colab":{}},"source":["def train(model, epochs, log_string):\n","    '''Train the RNN'''\n","    \n","    with tf.Session() as sess:\n","        sess.run(tf.global_variables_initializer())\n","\n","        # Used to determine when to stop the training early\n","        testing_loss_summary = []\n","\n","        # Keep track of which batch iteration is being trained\n","        iteration = 0\n","        \n","        display_step = 30 # The progress of the training will be displayed after every 30 batches\n","        stop_early = 0 \n","        stop = 9 # If the batch_loss_testing does not decrease in 3 consecutive checks, stop training\n","        per_epoch = 3 # Test the model 3 times per epoch\n","        testing_check = (len(training_sorted)//batch_size//per_epoch)-1\n","\n","        print()\n","        print(\"Training Model: {}\".format(log_string))\n","\n","        train_writer = tf.summary.FileWriter('./logs/1/train/{}'.format(log_string), sess.graph)\n","        test_writer = tf.summary.FileWriter('./logs/1/test/{}'.format(log_string))\n","\n","        for epoch_i in range(1, epochs+1): \n","            batch_loss = 0\n","            batch_time = 0\n","            \n","            for batch_i, (input_batch, target_batch, input_length, target_length) in enumerate(\n","                    get_batches(training_sorted, training_noisy, batch_size, threshold)):\n","                start_time = time.time()\n","\n","                summary, loss, _ = sess.run([model.merged,\n","                                             model.cost, \n","                                             model.train_op], \n","                                             {model.inputs: input_batch,\n","                                              model.targets: target_batch,\n","                                              model.inputs_length: input_length,\n","                                              model.targets_length: target_length,\n","                                              model.keep_prob: keep_probability})\n","\n","\n","                batch_loss += loss\n","                end_time = time.time()\n","                batch_time += end_time - start_time\n","\n","                # Record the progress of training\n","                train_writer.add_summary(summary, iteration)\n","\n","                iteration += 1\n","\n","                if batch_i % display_step == 0 and batch_i > 0:\n","                    print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n","                          .format(epoch_i,\n","                                  epochs, \n","                                  batch_i, \n","                                  len(training_sorted) // batch_size, \n","                                  batch_loss / display_step, \n","                                  batch_time))\n","                    batch_loss = 0\n","                    batch_time = 0\n","\n","                #### Testing ####\n","                if batch_i % testing_check == 0 and batch_i > 0:\n","                    batch_loss_testing = 0\n","                    batch_time_testing = 0\n","                    for batch_i, (input_batch, target_batch, input_length, target_length) in enumerate(\n","                            get_batches(testing_sorted, testing_noisy, batch_size, threshold)):\n","                        start_time_testing = time.time()\n","                        summary, loss = sess.run([model.merged,\n","                                                  model.cost], \n","                                                     {model.inputs: input_batch,\n","                                                      model.targets: target_batch,\n","                                                      model.inputs_length: input_length,\n","                                                      model.targets_length: target_length,\n","                                                      model.keep_prob: 1})\n","\n","                        batch_loss_testing += loss\n","                        end_time_testing = time.time()\n","                        batch_time_testing += end_time_testing - start_time_testing\n","\n","                        # Record the progress of testing\n","                        test_writer.add_summary(summary, iteration)\n","\n","                    n_batches_testing = batch_i + 1\n","                    print('Testing Loss: {:>6.3f}, Seconds: {:>4.2f}'\n","                          .format(batch_loss_testing / n_batches_testing, \n","                                  batch_time_testing))\n","                    \n","                    batch_time_testing = 0\n","\n","                    # If the batch_loss_testing is at a new minimum, save the model\n","                    testing_loss_summary.append(batch_loss_testing)\n","                    if batch_loss_testing <= min(testing_loss_summary):\n","                        print('New Record!') \n","                        stop_early = 0\n","                        checkpoint = \"./{}.ckpt\".format(log_string)\n","                        saver = tf.train.Saver()\n","                        saver.save(sess, checkpoint)\n","\n","                    else:\n","                        print(\"No Improvement.\")\n","                        stop_early += 1\n","                        if stop_early == stop:\n","                            break\n","\n","            if stop_early == stop:\n","                print(\"Stopping Training.\")\n","                break"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"YKiuPqbFb_Xd","colab_type":"code","outputId":"0958e239-c215-4fc5-bbda-39de47121277","executionInfo":{"status":"ok","timestamp":1585932306616,"user_tz":-180,"elapsed":46935,"user":{"displayName":"Ivan Torubarov","photoUrl":"","userId":"07481003931234110333"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# Train the model with the desired tuning parameters\n","for keep_probability in [0.75]:\n","    for num_layers in [2]:\n","        for threshold in [0.95]:\n","            log_string = 'kp={},nl={},th={}'.format(keep_probability,\n","                                                    num_layers,\n","                                                    threshold) \n","            model = build_graph(keep_probability, rnn_size, num_layers, batch_size, \n","                                learning_rate, embedding_size, direction)\n","            train(model, epochs, log_string)"],"execution_count":37,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","WARNING:tensorflow:From <ipython-input-27-dce37f4df695>:25: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:From <ipython-input-27-dce37f4df695>:37: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.add_weight` method instead.\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","\n","Training Model: kp=0.75,nl=2,th=0.95\n","Testing Loss:  3.127, Seconds: 0.73\n","New Record!\n","Epoch   1/100 Batch   30/49 - Loss:  3.089, Seconds: 3.99\n","Testing Loss:  2.315, Seconds: 0.46\n","New Record!\n","Testing Loss:  2.828, Seconds: 0.47\n","No Improvement.\n","Testing Loss:  1.586, Seconds: 0.44\n","New Record!\n","Epoch   2/100 Batch   30/49 - Loss:  1.955, Seconds: 2.71\n","Testing Loss:  1.308, Seconds: 0.54\n","New Record!\n","Testing Loss:  2.007, Seconds: 0.60\n","No Improvement.\n","Testing Loss:  1.194, Seconds: 0.57\n","New Record!\n","Epoch   3/100 Batch   30/49 - Loss:  1.267, Seconds: 2.82\n","Testing Loss:  1.798, Seconds: 0.62\n","No Improvement.\n","Testing Loss:  3.128, Seconds: 0.59\n","No Improvement.\n","Testing Loss:  1.715, Seconds: 0.58\n","No Improvement.\n","Epoch   4/100 Batch   30/49 - Loss:  0.972, Seconds: 2.92\n","Testing Loss:  2.461, Seconds: 0.59\n","No Improvement.\n","Testing Loss:  3.332, Seconds: 0.60\n","No Improvement.\n","Testing Loss:  2.977, Seconds: 0.57\n","No Improvement.\n","Epoch   5/100 Batch   30/49 - Loss:  0.806, Seconds: 3.17\n","Testing Loss:  3.983, Seconds: 0.62\n","No Improvement.\n","Testing Loss:  4.012, Seconds: 0.64\n","No Improvement.\n","Testing Loss:  1.410, Seconds: 0.57\n","No Improvement.\n","Stopping Training.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"YzwSIeNGb_Xi","colab_type":"text"},"source":["## Fixing Custom Sentences"]},{"cell_type":"code","metadata":{"id":"3nwMImkNb_Xj","colab_type":"code","colab":{}},"source":["def text_to_ints(text):\n","    '''Prepare the text for the model'''\n","    \n","    text = clean_text(text)\n","    return [vocab_to_int[word] for word in text]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hFndCM9gffCM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":175},"outputId":"be6e52fa-91a7-4aae-a776-58a4505d43a0","executionInfo":{"status":"ok","timestamp":1585932310145,"user_tz":-180,"elapsed":49927,"user":{"displayName":"Ivan Torubarov","photoUrl":"","userId":"07481003931234110333"}}},"source":["# Create your own sentence or use one from the dataset\n","text = \"перейдем\"\n","text = text_to_ints(text)\n","\n","#random = np.random.randint(0,len(testing_sorted))\n","#text = testing_sorted[random]\n","#text = noise_maker(text, 0.95)\n","\n","checkpoint = \"/content/gdrive/My Drive/RLC/best_obfuscator/best_obfuscator.ckpt\"\n","\n","model = build_graph(keep_probability, rnn_size, num_layers, batch_size, learning_rate, embedding_size, direction) \n","\n","with tf.Session() as sess:\n","    # Load saved model\n","    saver = tf.train.Saver()\n","    saver.restore(sess, checkpoint)\n","    \n","    #Multiply by batch_size to match the model's input parameters\n","    answer_logits = sess.run(model.predictions, {model.inputs: [text]*batch_size, \n","                                                 model.inputs_length: [len(text)]*batch_size,\n","                                                 model.targets_length: [len(text)+1], \n","                                                 model.keep_prob: [1.0]})[0]\n","\n","# Remove the padding from the generated sentence\n","pad = vocab_to_int[\"<PAD>\"] \n","\n","print('\\nText')\n","print('  Word Ids:    {}'.format([i for i in text]))\n","print('  Input Words: {}'.format(\"\".join([int_to_vocab[i] for i in text])))\n","\n","print('\\nSummary')\n","print('  Word Ids:       {}'.format([i for i in answer_logits if i != pad]))\n","print('  Response Words: {}'.format(\"\".join([int_to_vocab[i] for i in answer_logits if i != pad])))"],"execution_count":39,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/RLC/best_obfuscator/best_obfuscator.ckpt\n","\n","Text\n","  Word Ids:    [15, 6, 13, 6, 3, 24, 6, 7]\n","  Input Words: перейдем\n","\n","Summary\n","  Word Ids:       [15, 6, 13, 6, 24, 6, 7, 96]\n","  Response Words: передем<EOS>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ibiabFxBb_Xp","colab_type":"code","outputId":"0b0fd724-8f7d-4a6c-d99a-f977e3646f8d","executionInfo":{"status":"ok","timestamp":1585932313325,"user_tz":-180,"elapsed":52838,"user":{"displayName":"Ivan Torubarov","photoUrl":"","userId":"07481003931234110333"}},"colab":{"base_uri":"https://localhost:8080/","height":175}},"source":["# Create your own sentence or use one from the dataset\n","text = \"пойдешь\"\n","text = text_to_ints(text)\n","\n","#random = np.random.randint(0,len(testing_sorted))\n","#text = testing_sorted[random]\n","#text = noise_maker(text, 0.95)\n","\n","checkpoint = \"./kp=0.75,nl=2,th=0.95.ckpt\"\n","\n","model = build_graph(keep_probability, rnn_size, num_layers, batch_size, learning_rate, embedding_size, direction) \n","\n","with tf.Session() as sess:\n","    # Load saved model\n","    saver = tf.train.Saver()\n","    saver.restore(sess, checkpoint)\n","    \n","    #Multiply by batch_size to match the model's input parameters\n","    answer_logits = sess.run(model.predictions, {model.inputs: [text]*batch_size, \n","                                                 model.inputs_length: [len(text)]*batch_size,\n","                                                 model.targets_length: [len(text)+1], \n","                                                 model.keep_prob: [1.0]})[0]\n","\n","# Remove the padding from the generated sentence\n","pad = vocab_to_int[\"<PAD>\"] \n","\n","print('\\nText')\n","print('  Word Ids:    {}'.format([i for i in text]))\n","print('  Input Words: {}'.format(\"\".join([int_to_vocab[i] for i in text])))\n","\n","print('\\nSummary')\n","print('  Word Ids:       {}'.format([i for i in answer_logits if i != pad]))\n","print('  Response Words: {}'.format(\"\".join([int_to_vocab[i] for i in answer_logits if i != pad])))"],"execution_count":40,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from ./kp=0.75,nl=2,th=0.95.ckpt\n","\n","Text\n","  Word Ids:    [15, 8, 3, 24, 6, 5, 17]\n","  Input Words: пойдешь\n","\n","Summary\n","  Word Ids:       [15, 8, 24, 6, 5, 17, 96]\n","  Response Words: подешь<EOS>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6h3LFAapP8A5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"74a2853f-3d15-4ba3-d572-f2b637f4e239","executionInfo":{"status":"ok","timestamp":1585932321734,"user_tz":-180,"elapsed":60956,"user":{"displayName":"Ivan Torubarov","photoUrl":"","userId":"07481003931234110333"}}},"source":["if learn_noise:\n","  !mkdir /content/gdrive/My\\ Drive/RLC/best_obfuscator3\n","  !cp ./kp=0.75,nl=2,th=0.95.ckpt.data-00000-of-00001 /content/gdrive/My\\ Drive/RLC/best_obfuscator3/best_obfuscator.ckpt.data-00000-of-00001\n","  !cp ./kp=0.75,nl=2,th=0.95.ckpt.index /content/gdrive/My\\ Drive/RLC/best_obfuscator3/best_obfuscator.ckpt.index\n","  !cp ./kp=0.75,nl=2,th=0.95.ckpt.meta /content/gdrive/My\\ Drive/RLC/best_obfuscator3/best_obfuscator.ckpt.meta\n","else:\n","  !mkdir /content/gdrive/My\\ Drive/RLC/best_corrector\n","  !cp ./kp=0.75,nl=2,th=0.95.ckpt.data-00000-of-00001 /content/gdrive/My\\ Drive/RLC/best_corrector/best_corrector.ckpt.data-00000-of-00001\n","  !cp ./kp=0.75,nl=2,th=0.95.ckpt.index /content/gdrive/My\\ Drive/RLC/best_corrector/best_corrector.ckpt.index\n","  !cp ./kp=0.75,nl=2,th=0.95.ckpt.meta /content/gdrive/My\\ Drive/RLC/best_corrector/best_corrector.ckpt.meta"],"execution_count":41,"outputs":[{"output_type":"stream","text":["mkdir: cannot create directory ‘/content/gdrive/My Drive/RLC/best_obfuscator3’: File exists\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"Nqr1e4Jnb_Xv","colab_type":"text"},"source":["Examples of corrected sentences:\n","- Spellin is difficult, whch is wyh you need to study everyday.\n","- Spelling is difficult, which is why you need to study everyday.\n","\n","\n","- The first days of her existence in th country were vrey hard for Dolly. \n","- The first days of her existence in the country were very hard for Dolly.\n","\n","\n","- Thi is really something impressiv thaat we should look into right away! \n","- This is really something impressive that we should look into right away!"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"RBGrBPBSb_Xx","colab_type":"text"},"source":["## Summary"]},{"cell_type":"markdown","metadata":{"id":"v4pp2FOhb_X0","colab_type":"text"},"source":["I hope that you have found this project to be rather interesting and useful. The example sentences that I have presented above were specifically chosen, and the model will not always be able to make corrections of this quality. Given the amount of data that we are working with, this model still struggles. For it to be more useful, it would require far more training data, and additional parameter tuning. This parameter values that I have above worked best for me, but I expect there are even better values that I was not able to find.\n","\n","Thanks for reading!"]}]}